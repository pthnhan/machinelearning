\chapter{Kiến thức nền tảng}

\section{Quy trình của lớp các bài toán phân loại}
Để giải quyết một bài toán phân loại, ta cần phải hiểu dữ liệu, tính chất các đặc trưng và quá trình chọn các đặc trưng phù hợp với mô hình cũng cần phải có một quy trình rõ ràng.

Bước đầu tiên là xác định bài toán, nghĩa là xác định nhãn của bài toán và xây dựng tập dữ liệu (data collection). Phải đảm bảo là tập dữ liệu phải liên quan đến bài toán được mô hình hóa. Bước này rất quan trọng vì sẽ ảnh hưởng rất nhiều đến kết quả phân loại. Ngoài ra, chỉ những đặc trưng có thông tin hữu ích về bài toán là nên được sử dụng. Trong một số trường hợp, khi gặp khó khăn về kiến thức cũng như khả năng thu thập dữ liệu. Chúng ta có thể sử dụng phương pháp brute-force để thay thế. Brute-force là một phương pháp giải quyết vấn đề bằng cách thực hiện tất cả các giải pháp có thể có và chọn ra giải pháp tốt nhất. Nó được sử dụng khi không có một thuật toán cụ thể nào có thể giải quyết vấn đề hoặc khi không có đủ kiến thức về vấn đề để thiết kế một giải pháp tối ưu. Tuy nhiên, phương pháp này có thể rất tốn kém và thời gian, đặc biệt là đối với các vấn đề có kích thước lớn. Do vậy, trong trường hợp này sẽ có một lượng rất lớn các biến được đo, xử lý và thêm vào tập dữ liệu. Tuy nhiên, hi vọng, trong tương lại có thể tác được các đặc trưng tốt nhất và phù hợp nhất với bài toán.

Nếu vấn đề về dữ liệu có thể được giải quyết, chúng ta sẽ nên bước tiếp theo trong quy trình phân loại là tiền xử lý dữ liệu (data pre-processing). Ở bước này, vấn đề chính là thiếu dữ liệu (missing data) và dữ liệu ngoại lai (outlier) cần phải được xử lý. Có một vài phương pháp phân tích thóng kê \cite{aggarwal2001outlier, hodge2004survey} để có thể dử lý các vấn đề này. Hơn nữa, đây là bước mà số lượng các đặc trưng của bài toán có thể giảm đi bằng việc áp dụng thuật toán lựa chọn đặc trưng.

Vấn đề này có thể được giải quyết, đưa chúng ta đến bước tiếp theo trong quy trình phân loại: tiền xử lý dữ liệu. Ở giai đoạn này, các vấn đề chính như giá trị bị thiếu và phát hiện ngoại lệ nên được xử lý. Có nhiều phương pháp phân tích thống kê để giải quyết các vấn đề này [1, 26]. Ngoài ra, đây là giai đoạn trong vấn đề mà số lượng đặc trưng của vấn đề có thể được giảm bằng thuật toán lựa chọn đặc trưng.

Bước tiếp theo là lựa chọn thuật toán phân loại. Có rất nhiều các thuật tóa phân loại, và mặc dù các thuật toán rất đa dạng và khác nhau về ý tưởng, nhưng không dễ dàng chọn được thuật toán nào là tốt nhất cho một bài toán cụ thể. Do đó, việc thử nghiệm và so sánh một số thuật toán là một cách làm tương đối phổ biến, mục tiêu cuối cùng là lựa chọn được thuật toán cho kết quả tốt nhất \cite{kotsiantis2007supervised}.

Việc đánh giá các thuật toán phân loại thường dựa trên độ chính xác của việc dự đoán. Một kỹ thuật điển hình là chia dữ liệu thành hai phần để huấn luyện mô hình và sử dụng phần còn lại để kiểm tra độ chính xác. Tuy nhiên, quy trình này thường dẫn đến kết quả không tốt khi áp dụng vào tập dữ liệu bên ngoài. Do đó, để giảm thiểu sai số, một số kỹ thuật phức tạp hơn như kiểm tra chéo (cross-validation) \cite{kohavi1995study} có thể được sử dụng.

Để có cái nhìn tổng quan, chúng tôi xin trình bày theo sơ đồ khối của toàn bộ quy trình của một bài toán phân loại.

\begin{center}
	\begin{tikzpicture}[thick, scale=0.85, every node/.style={scale=0.9}]
		\path
			(0:0) coordinate (O)
			(0:3) coordinate (A)
			(0:6.5) coordinate (B)
			(0:10) coordinate (C)
			(0:13.5) coordinate (D)
			(0:17) coordinate (E)
			(D) ++ (-90:3) coordinate (F)
			(E) ++ (-90:5) coordinate (G)
			(G) ++ (-90:3) coordinate (H)
		;
		\draw 
			(O) circle (1)
			($(A)+(180:1.25)+(-90:0.5)$) rectangle ($(A)+(0:1.25)+(90:0.5)$)
			($(B)+(180:1.25)+(-90:0.5)$) rectangle ($(B)+(0:1.25)+(90:0.5)$)
			($(C)+(180:1.25)+(-90:1)$) rectangle ($(C)+(0:1.25)+(90:1)$)
			($(D)+(180:1.25)+(-90:0.5)$) rectangle ($(D)+(0:1.25)+(90:0.5)$)
			($(E)+(180:1.25)+(-90:0.5)$) rectangle ($(E)+(0:1.25)+(90:0.5)$)
			
			($(F)+(180:1.25)+(-90:0.5)$) rectangle ($(F)+(0:1.25)+(90:0.5)$)
			
			(G)+(180:1)--+(90:1)--+(0:1)--+(-90:1)--cycle
			
			($(H)+(180:1.25)+(-90:0.5)$) rectangle ($(H)+(0:1.25)+(90:0.5)$)
		;
		\path 
			(O) ++ (90:0.25) node{Xác định} ++ (-90:0.5) node{bài toán}
			
			(A) ++ (90:0.25) node{Xây dựng} ++ (-90:0.5) node{tập dữ liệu}
			
			(B) ++ (90:0.25) node{Tiền xử lý} ++ (-90:0.5) node{dữ liệu}
			
			(C) ++ (90:0.5) node{Lựa chọn} ++ (-90:0.5) node{mô hình} ++ (-90:0.5) node{phân loại}
			
			(D) ++ (90:0.25) node{Huấn luyện} ++ (-90:0.5) node{mô hình}
			
			(E) ++ (90:0.25) node{Đánh giá} ++ (-90:0.5) node{mô hình}
			
			(F) ++ (90:0.25) node{Điều chỉnh} ++ (-90:0.5) node{tham số}
			(G) ++ (90:0.25)node {Kiểm} ++ (-90:0.5)node{tra}
			(H) node {Kết thúc}
		;
		\draw[->]
		($(O) + (0:1)$) -- ($(A)+(180:1.25)$)
		;
		
		\draw[->] ($(A) + (0:1.25)$) -- ($(B)+(180:1.25)$);
		\draw[->] ($(A) + (-90:5)$) -- ($(A)+(-90:0.5)$);
		
		\draw[->] ($(B) + (0:1.25)$) -- ($(C)+(180:1.25)$);
		\draw[->] ($(B) + (-90:5)$) -- ($(B)+(-90:0.5)$);
		
		\draw[->] ($(C) + (0:1.25)$) -- ($(D)+(180:1.25)$);
		\draw[->] ($(C) + (-90:5)$) -- ($(C)+(-90:1)$);
		
		\draw[->] ($(D) + (0:1.25)$) -- ($(E)+(180:1.25)$);
		\draw[->] ($(D) + (-90:5)$) -- ($(F)+(-90:0.5)$);
		\draw[->] ($(F) + (90:0.5)$) -- ($(D)+(-90:0.5)$);
		
		\draw[->] ($(D) + (0:1.25)$) -- ($(E)+(180:1.25)$);
		\draw[->] ($(E) + (-90:0.5)$) -- ($(G)+(90:1)$);
		\draw[->] ($(G) + (-90:1)$) -- node[right]{Đạt} ($(H)+(90:0.5)$);
		
		\draw ($(A) + (-90:5)$) -- ($(D)+(-90:5)$) -- node[above]{Chưa đạt} ($(G)+(180:1)$);
	\end{tikzpicture}
\end{center}

Trong toàn bộ quy trình, nếu có bất kì bước nào không tốt, quy trình phải quy trở lại bước trước đó. Có nhiều nguyên nhân có thể ảnh hưởng đến hiếu suất của một bài toán phân loại \cite{kotsiantis2007supervised} như:
\begin{itemize}
	\item Đặc trưng phù hợp không được lựa chọn tốt.
	\item Tập dữ liệu không đủ, ít mẫu quan sát.
	\item Số lượng các đặc trưng quá nhiều.
	\item Kỹ thuật tiền xử lý dữ liệu chưa được tốt.
	\item Mô hình phân loiaj được chọn không phù hợp cho vấn đề hoặc cần điều chỉnh tham số.
\end{itemize}

Vì vậy, không thể chỉ ra rõ ràng bước nào trong quy trình cần trở lại. Tuy nhiên, mục tiêu cuối cùng là giải quyết bài toán phân loại đạt kết quả tốt nhất cho dữ liệu chưa được quan sát. Đây là bài toán khó, và mỗi bước thường được thực hiện trong thời gian dài, thông thường chúng ta cần phải liên tục thực hiện nhiều thử nghiệm mới để cải thiện khả năng dự đoán của mô hình.

\section{Lựa chọn đặc trưng}
Ngày nay, các phương pháp học máy xuất hiện ngày càng nhiều và rất mạnh mẽ để giải quyết các bài toán dữ liệu lớn. Các mô hình học máy phổ biến hiện nay như cây quyết định (decision tree), rừng ngẫu nhiên (random forest), SVM, KNN,\ldots đều là những mô hình mạnh mẽ, linh hoạt và có độ chính xác cao cả trong bài toán phân loại hay bài toán hồi quy. Tuy nhiên, bên cạnh việc áp dụng các mô hình học máy, chúng ta cần phải chuẩn hóa dữ liệu tốt, bởi vì dữ liệu là nguyên liệu để mô hình học máy học dựa trên đó. Kết quả của một bài toán sử dụng học máy có thể sẽ được cải thiện rõ rệt nếu có bước chuẩn bị dữ liệu tốt. Và việc lựa chọn đặc trưng là một kĩ thuật quan trọng bênh cạnh việc trích xuất đặc trưng hay biến đổi đặc trưng. Trong phạm vi của khóa luận này, chúng tôi chỉ xin nhắc lại khái quát về lựa chọn đặc trưng và các thuật toán liên quan.

\subsection{Lựa chọn đặc trưng là gì?}
Để xây dựng mô hình, chúng ta sẽ cần đến thông tin. Thông tin đến tự những bộ dữ liệu, nhưng với sự bùng nổ của dữ liệu lớn (bigdata), dữ liệu dường như trở nên quá nhiều, khiến việc xây dựng mô hình gặp nhiều khó khăn như tăng chi phí tính toán, quá nhiều đặc trưng có thể dẫn tới hiện tượng quá khớp (overfitting) - là hiện tượng mô hình hoạt động tốt trên tập huấn luyện (training set), nhưng tệ trên tập thử nghiệm (testing set), một số đặc trưng có thể gây nhiễu và làm giảm chất lượng mô hình,\ldots

Có rất nhiều thuật toán lựa chọn đặc trưng đã được phát triển từ rất lâu đến tận thời điểm hiện tại. Trong đó, lựa chọn tiến (forward feature selection), lựa chọn lùi (backward feature selection) và lựa chọn từng bước (stepwise feature selection) là ba thuật toán rất phổ biến. Trong phần này, chúng tôi sẽ tóm tắt lại ba kĩ thuật này, và trình bày chi tiết cách chúng tôi sử dụng trong các thuật toán \ref{alg:ffs}, \ref{alg:bfs}, và \ref{alg:sfs}.

Từ đây, để thuận tiện trong việc trình bày, chúng tôi sẽ sử dụng \lq\lq ffs\rq\rq~ để chỉ thuật toán lựa chọn tiến, \lq\lq bfs\rq\rq~ để chỉ thuật toán lựa chọn lùi và \lq\lq sfs\rq\rq~ để chỉ thuật toán lựa chọn từng bước.

\subsection{Thuật toán lựa chọn tiến}
Ffs được sử dụng rất rộng rãi vì sự hiệu quả trong việc tính toán của nó, cùng với khả năng xử lý hiệu quả các vấn đề bao gồm việc số lượng đặc trưng vượt quá số lượng quan sát. Tuy nhiên, một số đặc trưng có thể xuất hiện dư thừa sau khi đã lựa chọn các đặc trưng khác. Về các điều kiện đủ để lựa chọn tiến nhằm khôi phục mô hình ban đầu và tính ổn định của nó, chúng tôi tham khảo từ \cite{tropp2004greed} và \cite{donoho2005stable}. Dưới đây là chi tiết thuật toán

\begin{breakablealgorithm}
	\caption{\textbf{Lựa chọn tiến}\\
		(Forward Feature Selection)}\label{alg:ffs}
	\noindent\textbf{Input:} Một tập dữ liệu gồm $p$ đặc trưng $f_1,f_2,...,f_p$, tham số tiến là $\alpha$.\\
	\textbf{Output:} Một tập hợp $R$ gồm các đặc trưng được chọn.
	\begin{algorithmic}[1]
		\State $R \gets \emptyset$
		\State $S \gets \{f_1, f_2,\ldots, f_p\}$
		\While {True}
		\State $f_j \gets$ đặc trưng hữu ích nhất trong $S$
		\If{mô hình được cải thiện tốt hơn một lượng là $\alpha$ sau khi thêm vào $f_j$}
		\State $R \gets R \cup \{f_j\}$
		\State $S \gets S\setminus \{f_j\}$
		\Else
		\State \Return $R$
		\EndIf
		\EndWhile
	\end{algorithmic}
\end{breakablealgorithm}

\subsection{Thuật toán lựa chọn lùi}
Bfs \cite{james2013introduction} được trình bày chi tiết trong thuật toán \ref{alg:bfs}. Bắt đầu với toàn bộ đặc trưng, sau đó lần lượt loại bỏ các đặc trưng ít hữu ích nhất từ từ, mỗi lần một đặc trưng. Yêu cầu của bfs bảo đảm những đặc trưng dư thừa được loại bỏ khỏi mô hình. Tuy nhiên, bfs thì có tốc độ tính toán khá chậm, và chậm hơn nhiều khi so với fsf.

\begin{breakablealgorithm}
	\caption{\textbf{Lựa chọn lùi}\\
		(Backward Feature Selection)}\label{alg:bfs}
	\noindent\textbf{Input:} Một tập dữ liệu gồm $p$ đặc trưng $f_1,f_2,...,f_p$, tham số lùi là $\beta$.\\
	\textbf{Output:} Một tập hợp $R$ gồm các đặc trưng được chọn.
	\begin{algorithmic}[1]
		\State $R \gets \{f_1, f_2,\ldots, f_p\}$
		\While {True,}
		\State $f_j \gets$ là đặc trưng ít hữu ích nhất trong R.
		\If {giá trị mất mát của mô hình sau khi loại $f_j$ là nhỏ hơn $\beta$}
		\State $R \gets R\setminus \{f_j\}$
		\Else 
		\State \Return $R$
		\EndIf
		\EndWhile
	\end{algorithmic}
\end{breakablealgorithm} 

\subsection{Thuật toán lựa chọn từng bước}
Ngoài hai thuật toán được nêu ra ở trên, một thuật toán khác, là phiên bản kết hợp của cả thuật toán lựa chọn tiến và thuật toán lựa chọn lùi là thuật toán lựa chọn từng bước, được trình bày chi tiết trong thuật toán \ref{alg:sfs}. Trong cách tiếp cận này, các đặc trưng được thêm vào mô hình một cách có tuần tự như trong lựa chọn tiến. Tuy nhiên, sau khi thêm vào các đặc trưng mới, phương pháp này có thể loại bỏ bất kỳ đặc trưng nào mà có vẻ không còn phù hợp.

\begin{breakablealgorithm}
	\caption{\textbf{Lựa chọn từng bước}\\
		(Stepwise Feature Selection)}\label{alg:sfs}
	%\hspace*{\algorithmicindent} 
	\noindent\textbf{Input:} Một tập hợp gồm $p$ đặc trưng $f_1,f_2,...,f_p$, tham số tiến là $\alpha$, tham số lùi là $\beta$.\\
	\textbf{Output:} Một tập hợp $R$ gồm các đặc trưng được chọn.
	\begin{algorithmic}[1]
		\State $R \gets \emptyset$
		\State $S \gets \{f_1, f_2,\ldots, f_p\}$
		\While {True,}
		\State $f_j \gets$ là đặc trưng hữu ích nhất trong $S$
		\If{mô hình cải thiện hơn một lượng là $\alpha$ sau khi thêm $f_j$}
		\State $R \gets R \cup \{f_j\}$
		\State $S \gets S\setminus \{f_j\}$
		\While{True}
		\State {$f_k\gets$ là đặc trưng ít hữu ích nhất trong R}
		\If {hiệu suất của mô hình giảm một lượng nhỏ hơn $\beta$ sau khi loại bỏ $f_j$.}
		\State $R \gets R\setminus \{f_k\}$
		\Else {\;break}
		\EndIf
		\EndWhile\;
		\Else 
		\State{\;\Return $R$}
		\EndIf
		\EndWhile
	\end{algorithmic}
\end{breakablealgorithm}

\section{Các nghiên cứu liên quan}
Đối mặt với các thách thức về kích thước của dữ liệu ngày càng tăng, đã có nhiều nỗ lực trong hướng nghiên cứu về lựa chọn đặc trưng để phát triển các kỹ thuật mới. Bên cạnh các phương pháp lai để kết hợp các chiến lược lựa chọn đặc trưng khác nhau \cite{saeys2007review,ang2015supervised}, hầu hết các phương pháp lựa chọn đặc trưng có thể được chia thành ba loại.

Đầu tiên, cách tiếp cận \lq\lq wrapper\rq\rq~ dựa trên hiệu suât của một thuật toán học máy cụ thể để đánh giá tầm quan trọng của các đặc trưng được chọn. Một phương pháp \lq\lq wrapper\rq\rq~ điển hình sẽ tìm kiếm một tập con các đặc trưng dựa trên một thuật toán học máy trước, sau đó sẽ đánh giá chúng. Các bước này được lặp lại cho đến khi thỏa mãn một số tiêu chí dừng. Các phương pháp trong loại này thường rất tốn kém về chi phí tính toán vì việc đánh giá tập con các đặc trưng yêu cầu nhiều lần lặp lại. Mặc dùng nhiều các tiếp cận tìm kiếm được đề xuất chẳng hạn như thuật toán tìm kiếm best-first \cite{arai2016unsupervised} và thuật toán di chuyền (genetic) \cite{goldberg1988genetic}. Tuy nhiên, việc sử dụng các thuật toán này cho dữ liệu nhiều chiều vẫn không cho thấy sử cải thiện về chi phí tính toán.

Thứ hai, cách tiếp cận \lq\lq filter\rq\rq~ bao gồm các kỹ thuật đánh giá các tập hợp con đặc bằng việc xếp hạng với một số tiêu chí như tiêu chí thông tin \cite{nguyen2014efficiency, shishkin2016efficiency}, khả năng tái tạo \cite{farahat2011efficiency, masaeli2010convex}. Các phương pháp này chọn các đặc trưng độc lập với thuật toán học máy và thường hiệu quả hơn về chi phí tính toán so với các phương pháp \lq\lq wrapper\rq\rq~\cite{li2017feature}. Tuy nhiên, vì không được tối ưu hóa cho bất kỳ thuật toán học máy mục tiêu nào, nên chúng có thể không tối ưu cho một tuật toán học máy cụ thể.

Thứ ba, các phương pháp \lq\lq embedded\rq\rq~ sử dụng các tiêu chí độc lập để tìm ra tập con tối ưu cho một tập hợp nhất định. Sau đó, một thuật toán học máy được sử dụng để lựa chọn tập con tối ưu cuối cùng trong số các tập con tối ưu trên các tập hợp khác nhau. Vì thế, chúng hiệu quả hơn về chi phí tính toán so với các phương pháp \lq\lq wrapper\rq\rq~ vì chúng không đánh giá đặc trưng dựa trên việc lặp lại các tập con đặc trưng. Ngoài ra, chúng cũng được huấn luyện từ các thuật toán học máy. Vì thế, chúng có thể được coi như sự đánh đổi giữa phương pháp \lq\lq filter\rq\rq~ và phương pháp \lq\lq wrapper\rq\rq \cite{li2017feature}.

Mặc dù, cho đến này, các nhà khoa học đã nỗ lực rất nhiều trong hướng nghiên cứu lựa chọn đặc trưng, nhưng dữ liệu từ các trường, các ngành khác nhau có thể quá phong phú ngay cả đối với các phương pháp \lq\lq filter\rq\rq hiệu quả về chi phí tính toán. Điều này đã thúc đẩy nhiều nghiên cứu khác trong việc lựa chọn đặc trưng song song. Một số phương pháp đã được đề xuất trong \cite{melab2006grid,de2006parallelizing,garcia2006parallel,guillen2009efficiency,lopez2006solve} sử dụng quy trình xử lý song song để đánh giá đồng thời nhiều đặc trưng. Tuy nhiên, các thuật toán này yêu cầu quyên truy cập vào toàn bộ dữ liệu. Mặc khác, trong trong \cite{singh2009parallel}, các tác giả đã đề xuất một thuật toán lựa chọn đặc trưng song song cho hồi quy logistic dựa trên framework MapReduce và các đặc trưng được đánh giá thông qua hàm mục tiêu của mô hình hồi quy logistic. Trong khi đó, các tác giả của bài báo \cite{tsamardinos2019greedy} đã đề xuất \textit{Song song, Tiến–Lùi với thuật toán Tỉa (Parallel, Forward–Backward with Pruning algorithm)} (PFBP) để lựa chọn đặc trưng bằng cách bỏ sớm một số đặc trưng trong các lần lặp lại tiếp theo và sớm trả ra kết quả đặc trưng tốt nhất trong mỗi lần lặp. Tuy nhiên, các tiếp cận này yêu cầu tính toán bootstrap của p-giá trị, rất tốn kém chi phí tính toán. Trong bài báo \cite{zhao2013massively}, Zhao và các cộng sự đã giới thiệu một thuật toán lựa chọn đặc trưng song song để chọn các đặc trưng dựa trên khả năng giải thích phương sai của dữ liệu. Tuy nhiên, theo các tiếp cận của họ, việc xác định số lượng các đặc trưng trong mô hình dựa trên việc chuyển đổi các nhãn phân loại thành các giá trị số và sử dụng tổng bình phương sai số. Việc tính tổng bình phương sai số đòi hỏi phải điều chỉnh mô hình và do đó thuật toán vẫn còn tốn kém rất nhiều chi phí tính toán.